# Finetuning CSM

Let's fine-tune the CSM! Please note that the current implementation doesn't apply compute amortization, thus it might require a ton of RAM for large batch sizes! (Mac Studio recommended)

For the CLI version, please take a look at [FINETUNING_CLI.md](FINETUNING_CLI.md) instead.

Every API documented here is available in the `csm_mlx.finetune` module.

## Data Preparation

The finetuning `CSMTrainer` uses a dataset class `CSMDataset`. `CSMDataset` supports two initializers: `__init__` and `from_json`.

- `__init__` takes samples, which is a `list[list[Segment]]`.
- `from_json` takes a JSON file path that looks like the example below and loads the dataset from it.

```json
[
  [
    {"text": "Hello, how can I help you?", "audio_path": "/path/to/conversation1/turn1.wav", "speaker": 0},
    {"text": "I'd like to book a flight.", "audio_path": "/path/to/conversation1/turn2.wav", "speaker": 1},
    {"text": "Okay, where would you like to go?", "audio_path": "/path/to/conversation1/turn3.wav", "speaker": 0}
  ],
  [
    {"text": "Did you watch the game last night?", "audio_path": "/path/to/conversation2/turn1.wav", "speaker": 2},
    {"text": "Yeah, it was incredible!", "audio_path": "/path/to/conversation2/turn2.wav", "speaker": 3}
  ]
]
```

**Other arguments you can pass to the initializer**

- `n_audio_codebooks`: Number of audio codebooks to use. (Should be identical to model.n_audio_codebooks)
- `max_audio_length_ms`: Maximum audio length in milliseconds. (Audio will be cut off if exceeding)
- `mask_speaker_ids`: List of speaker IDs to mask. (To avoid training on specific speaker_ids, similar to masking_prompt in LLM training!)

## Training

We have `CSMTrainer`. The loss function is the standard cross-entropy loss adapted for the CSM architecture.

To initialize `CSMTrainer`, you need to use `TrainArgs`.

- `model`: The `CSM` model instance to be trained. Make sure to freeze all necessary layers before training. The trainer won't freeze any layers.
- `optimizer`: The [`mlx.optimizers`](https://ml-explore.github.io/mlx/build/html/python/optimizers.html) instance (e.g., `AdamW`).
- `output_dir`: A `Path` object pointing to the directory where checkpoints and logs will be saved.
- `first_codebook_weight_multiplier`: Multiplies the loss contribution of the first audio codebook. This exists since the first codebook is generated by the backbone, while others are generated by the decoder.
- `max_norm`: The maximum norm for gradient clipping. Helps prevent exploding gradients. Set to `0.0` to disable it!
- `gradient_checkpointing`: If `True`, enables gradient checkpointing on model layers to reduce memory usage at the cost of computation time. Useful for people with limited VRAM.
- `log_freq`: Log training loss and learning rate every N steps.
- `ckpt_freq`: Save a checkpoint every N steps.
- `only_save_trainable_params`: If `True`, only saves parameters returned by `model.trainable_parameters()`. Useful for LoRA where you only want to save the adapter weights, not the entire frozen base model.

> If you pass an `output_dir` which was previously used as a run directory, the `CSMTrainer` will try to resume from there!

## LoRA?

Use the `linear_to_lora_layers` function. It's specifically located in `csm_mlx.finetune.utils`. The API usage is the same as the `mlx_lm.tuner.linear_to_lora_layers` function. (Note that specifying `attn` will create LoRALinear for `q_proj`, `k_proj`, `v_proj`, `o_proj`, `up_proj`, `down_proj`, `gate_proj`). Remember to freeze the base model before converting to LoRALinear!

### Loading LoRA

If you trained a LoRA model, you can load it via the `load_adapters` function after loading CSM. Specify the run directory.

> If you're trying to run one of the checkpoints, rename the checkpoint to `adapters.safetensors` and copy the `adapter_config.json` from the parent folder. Then you should be able to run it!

## Todo

- [ ] Implement computation amortization
- [ ] Implement offline RL for better tuning (DPO, KTO)

Please let me know if there are any bugs or suggestions!
