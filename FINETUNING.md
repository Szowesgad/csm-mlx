# Finetuning CSM

Let's fine-tune the CSM! Please note that the current implementation doesn't apply compute amortization, thus it might require a ton of RAM for large batch sizes! (Mac Studio recommended)

For the CLI version, please take a look at [FINETUNING_CLI.md](FINETUNING_CLI.md) instead.

Every API documented here is available in the `csm_mlx.finetune` module.

## Data Preparation

The finetuning `CSMTrainer` uses a dataset class `CSMDataset`. `CSMDataset` supports two initializers: `__init__` and `from_json`.

- `__init__` takes samples, which is a `list[list[Segment]]`.
- `from_json` takes a JSON file path that looks like the example below and loads the dataset from it.

```json
[
  [
    {"text": "Hello, how can I help you?", "audio_path": "/path/to/conversation1/turn1.wav", "speaker": 0},
    {"text": "I'd like to book a flight.", "audio_path": "/path/to/conversation1/turn2.wav", "speaker": 1},
    {"text": "Okay, where would you like to go?", "audio_path": "/path/to/conversation1/turn3.wav", "speaker": 0}
  ],
  [
    {"text": "Did you watch the game last night?", "audio_path": "/path/to/conversation2/turn1.wav", "speaker": 2},
    {"text": "Yeah, it was incredible!", "audio_path": "/path/to/conversation2/turn2.wav", "speaker": 3}
  ]
]
```

**Other arguments you can pass to the initializer**

- `n_audio_codebooks`: Number of audio codebooks to use. (Should be identical to model.n_audio_codebooks)
- `max_audio_length_ms`: Maximum audio length in milliseconds. (Audio will be cut off if exceeding)
- `mask_speaker_ids`: List of speaker IDs to mask. (To avoid training on specific speaker_ids, similar to masking_prompt in LLM training!)

## Training

We have `CSMTrainer`. The loss function is the standard cross-entropy loss adapted for the CSM architecture.

To initialize `CSMTrainer`, you need to use `TrainArgs`.

- `model`: The `CSM` model instance to be trained. Make sure to freeze all necessary layers before training. The trainer won't freeze any layers.
- `optimizer`: The [`mlx.optimizers`](https://ml-explore.github.io/mlx/build/html/python/optimizers.html) instance (e.g., `AdamW`).
- `output_dir`: A `Path` object pointing to the directory where checkpoints and logs will be saved.
- `first_codebook_weight_multiplier`: Multiplies the loss contribution of the first audio codebook. This exists since the first codebook is generated by the backbone, while others are generated by the decoder.
- `max_norm`: The maximum norm for gradient clipping. Helps prevent exploding gradients. Set to `0.0` to disable it!
- `gradient_checkpointing`: If `True`, enables gradient checkpointing on model layers to reduce memory usage at the cost of computation time. Useful for people with limited VRAM.
- `log_freq`: Log training loss and learning rate every N steps.
- `ckpt_freq`: Save a checkpoint every N steps.
- `only_save_trainable_params`: If `True`, only saves parameters returned by `model.trainable_parameters()`. Useful for LoRA where you only want to save the adapter weights, not the entire frozen base model.

> If you pass an `output_dir` which was previously used as a run directory, the `CSMTrainer` will try to resume from there!

## LoRA?

Use the `linear_to_lora_layers` function. It's specifically located in `csm_mlx.finetune.utils`. The API usage is the same as the `mlx_lm.tuner.linear_to_lora_layers` function. (Note that specifying `attn` will create LoRALinear for `q_proj`, `k_proj`, `v_proj`, `o_proj`, `up_proj`, `down_proj`, `gate_proj`). Remember to freeze the base model before converting to LoRALinear!

### Loading LoRA

If you trained a LoRA model, you can load it via the `load_adapters` function after loading CSM. Specify the run directory.

> If you're trying to run one of the checkpoints, rename the checkpoint to `adapters.safetensors` and copy the `adapter_config.json` from the parent folder. Then you should be able to run it!

---

### Some script that might help you navigate

I'm planning to add those on CLI, meantime please use this.

### Generation from fine-tuned weight

To generate from full-finetuned model, you can just
```py
from mlx_lm.sample_utils import make_sampler
from csm_mlx import CSM, csm_1b, generate

import audiofile
import numpy as np

csm = CSM(csm_1b())
csm.load_weights("./finetuned_weight.safetensors")

audio = generate(
    csm,
    text="Hello from Sesame.",
    speaker=0,
    context=[],
    max_audio_length_ms=10_000,
    sampler=make_sampler(temp=0.8, top_k=50)
)

audiofile.write("./audio.wav", np.asarray(audio), 24000)
```

Just replace the `./finetuned_weight.safetensors` with your fine-tuned weight path, and it should work!

For LoRA tuned model, you can try:
```py
from mlx_lm.sample_utils import make_sampler
from huggingface_hub import hf_hub_download
from csm_mlx import CSM, csm_1b, generate, load_adapters

import audiofile
import numpy as np

csm = CSM(csm_1b())
weight = hf_hub_download(repo_id="senstella/csm-1b-mlx", filename="ckpt.safetensors")
csm.load_weights(weight)

# Load LoRA here!
load_adapters(csm, "./run-path")

audio = generate(
    csm,
    text="Hello from Sesame.",
    speaker=0,
    context=[],
    max_audio_length_ms=10_000,
    sampler=make_sampler(temp=0.8, top_k=50)
)

audiofile.write("./audio.wav", np.asarray(audio), 24000)
```

Replace the `./run-path` with your training path where it contains `adapters.safetensors` and `adapter_config.json` to ensure it works correctly!

#### Fusing the LoRA

In order to fuse the trained LoRA with original model to release in GitHub:

```py
from pathlib import Path
from csm_mlx import CSM, csm_1b, load_adapters
from huggingface_hub import hf_hub_download
from mlx.utils import tree_flatten, tree_unflatten
from mlx_lm.utils import save_weights

csm = CSM(csm_1b())
weight = hf_hub_download(repo_id="senstella/csm-1b-mlx", filename="ckpt.safetensors")
csm.load_weights(weight)

load_adapters(csm, './run-path')

fused_linears = [
    (n, m.fuse()) for n, m in csm.named_modules() if hasattr(m, "fuse")
]
csm.update_modules(tree_unflatten(fused_linears))

weights = dict(tree_flatten(csm.parameters()))
save_path = Path("./fused-weight")
save_weights(save_path, weights)
```

#### Converting model back to PyTorch mainline compatable:

If you want to convert the trained model to original PyTorch model compatible, you can try this (Must have `torch` installed):

```py
import torch
from safetensors.torch import load_file

tensors = load_file("ckpt.safetensors")
recovered_tensors = {}
for name, value in tensors.items():
    name = (
        name.replace("up_proj", "w3")
             .replace("down_proj", "w2")
             .replace("gate_proj", "w1")
             .replace("self_attn", "attn")
             .replace("norm.weight", "norm.scale")
             .replace("input_layernorm", "sa_norm")
             .replace("post_attention_layernorm", "mlp_norm")
             .replace("o_proj", "output_proj")
    )
    value = value.to(torch.float32)
    recovered_tensors[name] = value
    print(name, value.shape)

torch.save(recovered_tensors, "pytorch-ckpt.pt")
```

## Todo

- [ ] Implement computation amortization
- [ ] Implement offline RL for better tuning (DPO, KTO)

Please let me know if there are any bugs or suggestions!
